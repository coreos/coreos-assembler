#!/usr/bin/python3

# The inverse of cmd-buildprep (i.e. we upload a build which later can be
# partially re-downloaded with cmd-buildprep).

import argparse
import json
import os
import sys
import tempfile
import subprocess
import boto3
from botocore.exceptions import ClientError
from tenacity import retry

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# set image artifact caching at 1y; it'll probably get evicted before that...
# see also: https://stackoverflow.com/questions/2970938
CACHE_MAX_AGE_ARTIFACT = 60 * 60 * 24 * 365

# set metadata caching to 5m
CACHE_MAX_AGE_METADATA = 60 * 5
from cosalib.builds import Builds, BUILDFILES
from cosalib.cmdlib import load_json, retry_stop, retry_s3_exception, retry_callback  # noqa: E402


def main():
    args = parse_args()
    args.func(args)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--build", help="Build ID", default='latest')
    parser.add_argument("--dry-run", help="Just print and exit",
                        action='store_true')
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--skip-builds-json", help="Don't push builds.json",
                       action='store_true')

    subparsers = parser.add_subparsers(dest='cmd', title='subcommands')
    subparsers.required = True

    s3 = subparsers.add_parser('s3', help='upload an image')
    s3.add_argument("url", metavar='<BUCKET>[/PREFIX]',
                    help="Bucket and path prefix in which to upload")
    s3.add_argument("--acl", help="ACL for objects",
                    action='store', default='private')
    s3.add_argument("--enable-gz-peel", help="Auto-peel .gz extensions "
                    "and set Content-Disposition names", action='store_true')
    s3.set_defaults(func=cmd_upload_s3)

    return parser.parse_args()


def cmd_upload_s3(args):
    bucket, prefix = args.url.split('/', 1)
    builds = Builds()
    # This can't be an error for backcompat reasons, but let's print something
    if not os.path.isfile(BUILDFILES['sourceurl']):
        print(f"NOTICE: No {BUILDFILES['sourceurl']} file; uploading without buildprep?")
    if args.build == 'latest':
        args.build = builds.get_latest()
    print(f"Targeting build: {args.build}")
    for arch in builds.get_build_arches(args.build):
        s3_upload_build(args, builds.get_build_dir(args.build, arch),
                        bucket, f'{prefix}/{args.build}/{arch}')
    # if there's anything else in the build dir, just upload it too,
    # e.g. pipelines might inject additional metadata
    for f in os.listdir(f'builds/{args.build}'):
        # arches already uploaded higher up
        if f in builds.get_build_arches(args.build):
            continue
        # assume it's metadata
        s3_copy(f'builds/{args.build}/{f}', bucket, f'{prefix}/{args.build}/{f}',
                CACHE_MAX_AGE_METADATA, args.acl)
    if not args.skip_builds_json:
        s3_copy(BUILDFILES['list'], bucket, f'{prefix}/builds.json',
                CACHE_MAX_AGE_METADATA, args.acl, extra_args={}, dry_run=args.dry_run)
        # And now update our cached copy to note we've successfully sync'd.
        with open(BUILDFILES['sourceurl'], 'w') as f:
            f.write(f"s3://{bucket}/{prefix}\n")
        subprocess.check_call(['cp', '-pf', '--reflink=auto', BUILDFILES['list'], BUILDFILES['sourcedata']])


def s3_upload_build(args, builddir, bucket, prefix):
    build = load_json(f'{builddir}/meta.json')

    # Upload images with special handling for gzipped data.
    uploaded = set()
    for imgname in build['images']:
        img = build['images'][imgname]
        bn = img['path']
        path = os.path.join(builddir, bn)
        s3_path = f'{prefix}/{bn}'
        set_content_disposition = False

        # Don't use the Content-Disposition trick with bare-metal images since
        # the installer expects them gzipped. (This is a trick used to allow
        # recommending `curl -J --compressed` so that images are stored
        # compressed, but uncompressed on-the-fly at download time.)
        if (bn.endswith('.gz') and not bn.endswith('.raw.gz') and
                args.enable_gz_peel):
            nogz = bn[:-3]
            img['path'] = nogz
            s3_path = f'{prefix}/{nogz}'
            set_content_disposition = True

        if not os.path.exists(path):
            if s3_check_exists(bucket, s3_path):
                continue
            else:
                raise Exception(f"{path} not found locally or in the s3 destination!")

        extra_args = {}
        if set_content_disposition:
            extra_args = {
                'ContentEncoding': 'gzip',
                'ContentDisposition': f'inline; filename={img["path"]}'
            }
        s3_copy(path, bucket, s3_path,
            CACHE_MAX_AGE_ARTIFACT, args.acl,
            extra_args=extra_args,
            dry_run=args.dry_run)
        uploaded.add(bn)

    for f in os.listdir(builddir):
        # we do meta.json right after
        if f in uploaded or f == 'meta.json':
            continue
        path = os.path.join(builddir, f)
        s3_copy(path, bucket, f'{prefix}/{f}',
            CACHE_MAX_AGE_ARTIFACT, args.acl,
            extra_args={},
            dry_run=args.dry_run)

    # Now upload a modified version of the meta.json which has the fixed
    # filenames without the .gz suffixes. We don't want to modify the local
    # build dir.
    with tempfile.NamedTemporaryFile('w') as f:
        json.dump(build, f, indent=4)
        f.flush()
        s3_copy(f.name, bucket, f'{prefix}/meta.json',
            CACHE_MAX_AGE_METADATA, args.acl,
            extra_args={
                'ContentType': 'application/json'
            },
            dry_run=args.dry_run)


@retry(stop=retry_stop, retry=retry_s3_exception, before_sleep=retry_callback)
def s3_check_exists(bucket, key):
    print(f"Checking if bucket '{bucket}' has key '{key}'")
    s3 = boto3.client('s3')
    try:
        s3.head_object(Bucket=bucket, Key=key)
    except ClientError as e:
        if e.response['Error']['Code'] == '404':
            return False
        raise e
    return True


@retry(stop=retry_stop, retry=retry_s3_exception, retry_error_callback=retry_callback)
def s3_copy(src, bucket, key, max_age, acl, extra_args={}, dry_run=False):
    if key.endswith('.json') and 'ContentType' not in extra_args:
        extra_args['ContentType'] = 'application/json'
    upload_args = {
        'CacheControl': f'max-age={max_age}',
        'ACL': acl
    }
    upload_args.update(extra_args)
    s3 = boto3.client('s3')
    print(f"{'Would upload' if dry_run else 'Uploading'} {src} to s3://{bucket}/{key} {extra_args if len(extra_args) else ''}")
    if not dry_run:
        s3.upload_file(Filename=src, Bucket=bucket, Key=key, ExtraArgs=upload_args)


if __name__ == '__main__':
    sys.exit(main())
