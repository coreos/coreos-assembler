#!/usr/bin/python3

# The inverse of cmd-buildprep (i.e. we upload a build which later can be
# partially re-downloaded with cmd-buildprep).

import argparse
import json
import os
import subprocess
import sys
import tempfile

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from cmdlib import load_json, Builds  # noqa: E402

# set image artifact caching at 1y; it'll probably get evicted before that...
# see also: https://stackoverflow.com/questions/2970938
CACHE_MAX_AGE_ARTIFACT = 60 * 60 * 24 * 365

# set metadata caching to 5m
CACHE_MAX_AGE_METADATA = 60 * 5


def main():
    args = parse_args()
    args.func(args)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--build", help="Build ID", default='latest')
    parser.add_argument("--dry-run", help="Just print and exit",
                        action='store_true')
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--freshen", help="Only push builds.json",
                       action='store_true')
    group.add_argument("--skip-builds-json", help="Don't push builds.json",
                       action='store_true')

    subparsers = parser.add_subparsers(dest='cmd', title='subcommands')
    subparsers.required = True

    s3 = subparsers.add_parser('s3', help='upload an image')
    s3.add_argument("url", metavar='<BUCKET>[/PREFIX]',
                    help="Bucket and path prefix in which to upload")
    s3.add_argument("--acl", help="ACL for objects",
                    action='store', default='private')
    s3.add_argument("--enable-gz-peel", help="Auto-peel .gz extensions "
                    "and set Content-Disposition names", action='store_true')
    s3.set_defaults(func=cmd_upload_s3)

    return parser.parse_args()


def cmd_upload_s3(args):
    if not args.freshen:
        builds = Builds()
        if args.build == 'latest':
            args.build = builds.get_latest()
        print(f"Targeting build: {args.build}")
        if builds.is_legacy():
            s3_upload_build(args, builds.get_build_dir(args.build), args.build)
        else:
            for arch in builds.get_build_arches(args.build):
                s3_upload_build(args, builds.get_build_dir(args.build, arch),
                                f'{args.build}/{arch}')
            # if there's anything else in the build dir, just upload it too,
            # e.g. pipelines might inject additional metadata
            for f in os.listdir(f'builds/{args.build}'):
                # arches already uploaded higher up
                if f in builds.get_build_arches(args.build):
                    continue
                # assume it's metadata
                s3_cp(args, CACHE_MAX_AGE_METADATA,
                      f'builds/{args.build}/{f}', f'{args.build}/{f}')
    if not args.skip_builds_json:
        s3_cp(args, CACHE_MAX_AGE_METADATA,
              'builds/builds.json', 'builds.json')


def s3_upload_build(args, builddir, dest):
    build = load_json(f'{builddir}/meta.json')

    # Upload images with special handling for gzipped data.
    uploaded = set()
    for imgname in build['images']:
        img = build['images'][imgname]
        bn = img['path']
        path = os.path.join(builddir, bn)
        s3_path = f'{dest}/{bn}'
        set_content_disposition = False

        # Don't use the Content-Disposition trick with bare-metal images since
        # the installer expects them gzipped. (This is a trick used to allow
        # recommending `curl -J --compressed` so that images are stored
        # compressed, but uncompressed on-the-fly at download time.)
        if (bn.endswith('.gz') and not bn.endswith('.raw.gz') and
                args.enable_gz_peel):
            nogz = bn[:-3]
            img['path'] = nogz
            s3_path = f'{dest}/{nogz}'
            set_content_disposition = True

        if not os.path.exists(path):
            if s3_check_exists(args, s3_path):
                continue
            else:
                raise Exception(f"{path} not found locally or in the s3 destination!")

        if set_content_disposition:
            s3_cp(args, CACHE_MAX_AGE_ARTIFACT, path, s3_path,
                  '--content-encoding=gzip',
                  f'--content-disposition=inline; filename={img["path"]}')
        else:
            s3_cp(args, CACHE_MAX_AGE_ARTIFACT, path, s3_path)
        uploaded.add(bn)

    for f in os.listdir(builddir):
        # we do meta.json right after
        if f in uploaded or f == 'meta.json':
            continue
        path = os.path.join(builddir, f)
        s3_cp(args, CACHE_MAX_AGE_ARTIFACT, path, f'{dest}/{f}')

    # Now upload a modified version of the meta.json which has the fixed
    # filenames without the .gz suffixes. We don't want to modify the local
    # build dir.
    with tempfile.NamedTemporaryFile('w') as f:
        json.dump(build, f, indent=4)
        f.flush()
        s3_cp(args, CACHE_MAX_AGE_METADATA, f.name, f'{dest}/meta.json',
              '--content-type=application/json')


def s3_check_exists(args, path):
    path = f'{args.url}/{path}'
    bucket, key = path.split("/", 1)
    s3_args = ['aws', 's3api', 'head-object', '--bucket', bucket, '--key', key]
    return subprocess.call(s3_args, stdout=subprocess.DEVNULL) == 0


def s3_cp(args, max_age, src, dest, *s3_args):
    acl = f'--acl={args.acl}'
    max_age = f'--cache-control=max-age={max_age}'
    dest = f's3://{args.url}/{dest}'
    s3_args = ['aws', 's3', 'cp', acl, src, dest, max_age, *s3_args]
    print("%s: %s" % ("Would run" if args.dry_run else "Running",
                      subprocess.list2cmdline(s3_args)))
    if not args.dry_run:
        subprocess.check_call(s3_args, stdout=subprocess.DEVNULL)


if __name__ == '__main__':
    sys.exit(main())
