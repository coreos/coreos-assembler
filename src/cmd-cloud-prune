#!/usr/bin/python3 -u

# This script parses a policy.yaml file, which outlines the specific
# pruning actions required for each stream and the age threshold for
# deleting artifacts within them.
# Example of policy.yaml
# rawhide:
#     # all cloud images
#     cloud-uploads: 2y
#     # artifacts in meta.json's `images` key
#     images: 2y
#     images-keep: [qemu, live-iso]
#     build: 3y
#     containers: 2w
# The script also updates the builds.json for the respective stream by
# adding the policy-cleanup key when we set the upload_builds_json flag.
# It adds the relevant actions completed to that key
# For eg:
#     "builds": [
#         {
#             "id": "40.20240425.dev.1",
#             "arches": [
#                 "x86_64"
#             ],
#             "policy-cleanup": {
#                 "cloud-uploads": true,
#                 "images": true,
#                 "images-kept": ["qemu", "live-iso"]
#             }
#         }
#
# We should also prune unreferenced build directories here. See also
# `get_unreferenced_s3_builds()` in the git log

import argparse
import json
from urllib.parse import urlparse
import pytz
import yaml
import collections
import datetime
import os
import boto3
import botocore
from dateutil.relativedelta import relativedelta
from cosalib.gcp import remove_gcp_image
from cosalib.aws import deregister_aws_resource
from cosalib.builds import BUILDFILES
from cosalib.s3 import s3_copy
from cosalib.cmdlib import parse_fcos_version_to_timestamp_and_stream
from cosalib.cmdlib import convert_duration_to_days

Build = collections.namedtuple("Build", ["id", "images", "arch", "meta_json"])
# set metadata caching to 5m
CACHE_MAX_AGE_METADATA = 60 * 5
# These lists are up to date as of schema hash
# 4bb2fd8591ae1cc9179181b9efcb550542137a01c9544fd048e66794e0fb176a. If changing
# this hash, ensure that the list of SUPPORTED and UNSUPPORTED artifacts below
# is up to date.
SUPPORTED = ["amis", "gcp"]
UNSUPPORTED = ["aliyun", "azure", "ibmcloud", "powervs"]


def parse_args():
    parser = argparse.ArgumentParser(prog="coreos-assembler cloud-prune")
    parser.add_argument("--policy", required=True, type=str, help="Path to policy YAML file")
    parser.add_argument("--dry-run", help="Don't actually delete anything", action='store_true')
    parser.add_argument("--upload-builds-json", help="Push builds.json", action='store_true')
    parser.add_argument("--stream", type=str, help="CoreOS stream", required=True)
    parser.add_argument("--gcp-json-key", help="GCP Service Account JSON Auth", default=os.environ.get("GCP_JSON_AUTH"))
    parser.add_argument("--acl", help="ACL for objects", action='store', default='private')
    parser.add_argument("--aws-config-file", default=os.environ.get("AWS_CONFIG_FILE"), help="Path to AWS config file")
    return parser.parse_args()


def main():
    # Parse arguments and initialize variables
    args = parse_args()
    with open(BUILDFILES['sourceurl'], "r") as file:
        builds_source_data_url = file.read()
    bucket, prefix = get_s3_bucket_and_prefix(builds_source_data_url)
    cloud_config = get_cloud_config(args)
    stream = args.stream
    today_date = datetime.datetime.now()

    # Boto3 loads credentials from ~/.aws/config by default and we can change
    # this default location by setting the AWS_CONFIG_FILE environment variable.
    # The Python bindings don't support passing a config file.
    # The alternative is to manually pass ACCESS_KEY and SECRET_KEY which isn't favourable.
    if args.aws_config_file:
        os.environ["AWS_CONFIG_FILE"] = args.aws_config_file
    s3_client = boto3.client("s3")

    # Upload builds.json to s3 bucket
    if args.upload_builds_json:
        # This copies the local builds.json and updates the S3 bucket version.
        return handle_upload_builds_json(s3_client, bucket, prefix, args.dry_run, args.acl)

    with open(args.policy, "r") as f:
        policy = yaml.safe_load(f)
    if stream in policy:
        validate_policy(stream, policy)
    else:
        print(f"There is no policy defined in gc-policy.yaml for {stream}")
        return

    with open(BUILDFILES['list'], "r") as f:
        builds_json_data = json.load(f)
    # Original list of builds
    builds = builds_json_data["builds"]

    # Prune builds based on the policy
    for action in ['cloud-uploads', 'images', 'build']:
        if action not in policy[stream]:
            continue
        duration = convert_duration_to_days(policy[stream][action])
        ref_date = today_date - relativedelta(days=int(duration))
        pruned_build_ids = []
        images_to_keep = policy.get(stream, {}).get("images-keep", [])

        print(f"Pruning resources of type {action} older than {policy[stream][action]} ({ref_date.date()}) on stream {stream}")
        # Enumerating in reverse to go from the oldest build to the newest one
        for build in reversed(builds):
            build_id = build["id"]
            (build_date, _) = parse_fcos_version_to_timestamp_and_stream(build_id)
            if build_date >= ref_date:
                break

            previous_cleanup = build.get("policy-cleanup", {})
            if action in previous_cleanup:
                # If we are in here then there has been some previous cleanup of
                # this type run for this build. For all types except `images` we
                # can just continue.
                if action != "images":
                    print(f"Build {build_id} has already had {action} pruning completed")
                    continue
                else:
                    # OK `images` has been pruned before, but we need to check
                    # that all the images were pruned that match the current policy.
                    # i.e. there may be additional images we need prune
                    previous_images_kept = previous_cleanup.get("images-kept", [])
                    if set(images_to_keep) == set(previous_images_kept):
                        print(f"Build {build_id} has already had {action} pruning completed")
                        continue

            for arch in build["arches"]:
                print(f"Pruning {arch} {action} for {build_id}")
                meta_prefix = os.path.join(prefix, f"{build_id}/{arch}/meta.json")
                meta_json = get_json_from_s3(s3_client, bucket, meta_prefix)
                # Make sure the meta.json doesn't contain any cloud_platform that is not supported for pruning yet.
                images = get_supported_images(meta_json)
                current_build = Build(id=build_id, images=images, arch=arch, meta_json=meta_json)

                match action:
                    case "cloud-uploads":
                        prune_cloud_uploads(current_build, cloud_config, args.dry_run)
                    # Prune through images that are not mentioned in images-keep
                    case "images":
                        prune_images(s3_client, current_build, images_to_keep, args.dry_run, bucket, prefix)
                    # Fully prune releases that are very old including deleting the directory in s3 for that build.
                    case "build":
                        prune_build(s3_client, bucket, prefix, build_id, args.dry_run)
                        pruned_build_ids.append(build_id)
            # Update policy-cleanup after processing all arches for the build
            policy_cleanup = build.setdefault("policy-cleanup", {})
            match action:
                case "cloud-uploads":
                    if "cloud-uploads" not in policy_cleanup:
                        policy_cleanup["cloud-uploads"] = True
                case "images":
                    if "images" not in policy_cleanup:
                        policy_cleanup["images"] = True
                    policy_cleanup["images-kept"] = images_to_keep

    if pruned_build_ids:
        if "tombstone-builds" not in builds_json_data:
            builds_json_data["tombstone-builds"] = []
        # Separate the builds into remaining builds and tombstone builds
        remaining_builds = [build for build in builds if build["id"] not in pruned_build_ids]
        tombstone_builds = [build for build in builds if build["id"] in pruned_build_ids]
        # Update the data structure
        builds_json_data["builds"] = remaining_builds
        builds_json_data["tombstone-builds"].extend(tombstone_builds)

    # Save the updated builds.json to local builds/builds.json
    save_builds_json(builds_json_data, BUILDFILES['list'])


def get_s3_bucket_and_prefix(builds_source_data_url):
    parsed_url = urlparse(builds_source_data_url)
    if parsed_url.scheme == "s3":
        bucket, prefix = parsed_url.netloc, parsed_url.path.lstrip("/")
        return bucket, prefix
    raise Exception("Invalid scheme: only s3:// supported")


def get_cloud_config(args):
    return {
        "gcp": {
            "json-key": args.gcp_json_key,
        },
        "aws": {
            "credentials": args.aws_config_file
        }
    }


def validate_policy(stream, policy):
    # If the build key is set in the policy file, then the cloud-uploads key must
    # also be present, and the duration of cloud-uploads must be equal or shorter
    if "build" in policy[stream]:
        actions = policy[stream]
        if 'cloud-uploads' not in actions:
            raise Exception("Pruning for cloud-uploads must be set before we prune the builds")
        cloud_uploads_duration = convert_duration_to_days(actions["cloud-uploads"])
        build_duration = convert_duration_to_days(actions["build"])
        if cloud_uploads_duration > build_duration:
            raise Exception("Duration of pruning cloud-uploads must be less than or equal to pruning a build")


def get_supported_images(meta_json):
    images = {}
    for key in meta_json:
        if key in UNSUPPORTED:
            raise Exception(f"The platform {key} is not supported")
        if key in SUPPORTED:
            images[key] = meta_json[key]
    return images


def get_json_from_s3(s3, bucket, key):
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        content = response["Body"].read().decode("utf-8")
        return json.loads(content)
    except Exception as e:
        raise Exception(f"Error fetching the JSON file from S3 {bucket}/{key}: {e}")


def save_builds_json(builds_json_data, location):
    builds_json_data["timestamp"] = datetime.datetime.now(pytz.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    with open(location, "w") as json_file:
        json.dump(builds_json_data, json_file, indent=4)


def handle_upload_builds_json(s3_client, bucket, prefix, dry_run, acl):
    remote_builds_json = get_json_from_s3(s3_client, bucket, os.path.join(prefix, "builds.json"))
    # This is the copy of builds.json from what we last downloaded from the source
    with open(BUILDFILES['sourcedata'], "r") as f:
        builds_json_source_data = json.load(f)
    # This is the current list of builds at builds/builds.json
    with open(BUILDFILES['list'], "r") as f:
        current_builds_json = json.load(f)

    # If there are no changes to the local builds/builds.json we won't need to upload
    # anything to the s3 bucket. Will return in this scenario.
    if builds_json_source_data.get('builds') == current_builds_json.get('builds'):
        print("There are no changes to the local builds/builds.json. No upload needed")
        return

    # Check if there are any changes that were made to remote(s3 version) builds.json
    # while the pruning was in progress
    if remote_builds_json != builds_json_source_data:
        # Before we merge the changes, let's update the local tmp/builds-source.json with the latest remote_builds_json
        save_builds_json(remote_builds_json, BUILDFILES['sourcedata'])
        print("Detected remote updates to builds.json. Merging it to the local builds.json file")
        remote_builds_json = update_policy_cleanup(current_builds_json, remote_builds_json)
        # Make sure we have the merged json as local builds/builds.json
        save_builds_json(remote_builds_json, BUILDFILES['list'])

    # Print the updated builds.json before the s3 update
    with open(BUILDFILES['list'], 'r') as file:
        updated_builds_json = json.load(file)
    print("----")
    print(json.dumps(updated_builds_json, indent=4))
    print("----")

    with open(BUILDFILES['sourcedata'], 'r') as file:
        builds_json_source_data = json.load(file)
    # Make sure the size of the builds+tombstone-builds array is the same in the original and our modified builds.json
    source_builds_count = len(builds_json_source_data.get('builds', [])) + len(builds_json_source_data.get('tombstone-builds', []))
    updated_builds_count = len(updated_builds_json.get('builds', [])) + len(updated_builds_json.get('tombstone-builds', []))
    assert source_builds_count == updated_builds_count

    # Before uploading builds.json, copy the updated tmp/builds-source.json as builds.json.bak as a backup
    s3_copy(s3_client, BUILDFILES['sourcedata'], bucket, f'{prefix}/builds.json.bak', CACHE_MAX_AGE_METADATA, acl, extra_args={}, dry_run=dry_run)

    # Upload the local builds.json to s3
    return s3_copy(s3_client, BUILDFILES['list'], bucket, f'{prefix}/builds.json', CACHE_MAX_AGE_METADATA, acl, extra_args={}, dry_run=dry_run)


# Function to update policy-cleanup keys into remote_builds
def update_policy_cleanup(current_builds, remote_builds):
    current_builds_dict = {build['id']: build for build in current_builds['builds']}
    for remote_build in remote_builds['builds']:
        build_id = remote_build['id']
        if build_id in current_builds_dict:
            current_build = current_builds_dict[build_id]
            if 'policy-cleanup' in current_build:
                remote_build['policy-cleanup'] = current_build['policy-cleanup']
    return remote_builds


def prune_cloud_uploads(build, cloud_config, dry_run):
    # Ensure AWS AMIs and GCP images are removed based on the configuration
    errors = []
    errors.extend(deregister_aws_amis(build, cloud_config, dry_run))
    errors.extend(delete_gcp_image(build, cloud_config, dry_run))

    if errors:
        print(f"Found errors when removing cloud-uploads for {build.id}:")
        for e in errors:
            print(e)
        raise Exception("Some errors were encountered")


def deregister_aws_amis(build, cloud_config, dry_run):
    errors = []
    aws_credentials = cloud_config.get("aws", {}).get("credentials")
    amis = build.images.get("amis")
    if not amis:
        print(f"No AMI/Snapshot to prune for {build.id} for {build.arch}")
        return errors
    for ami in amis:
        region_name = ami.get("name")
        ami_id = ami.get("hvm")
        # If we are dealing with an old manifest where the snapshot ID isn't stored
        # then let's instruct ore to detect the snapshot ID from the AMI.
        snapshot_id = ami.get("snapshot", "detectFromAMI")
        if dry_run:
            print(f"Would delete {ami_id} and {snapshot_id} for {build.id}")
            continue
        if (ami_id or snapshot_id) and region_name:
            try:
                deregister_aws_resource(ami_id, snapshot_id, region=region_name, credentials_file=aws_credentials)
            except Exception as e:
                errors.append(e)
        else:
            errors.append(f"Missing parameters to remove {ami_id} and {snapshot_id}")
    return errors


def delete_gcp_image(build, cloud_config, dry_run):
    errors = []
    gcp = build.images.get("gcp")
    if not gcp:
        print(f"No GCP image to prune for {build.id} for {build.arch}")
        return errors
    gcp_image = gcp.get("image")
    project = gcp.get("project") or "fedora-coreos-cloud"
    json_key = cloud_config.get("gcp", {}).get("json-key")
    if dry_run:
        print(f"Would delete {gcp_image} GCP image for {build.id}")
    elif gcp_image and json_key and project:
        try:
            remove_gcp_image(gcp_image, json_key, project)
        except Exception as e:
            errors.append(e)
    else:
        errors.append(f"Missing parameters to remove {gcp_image}")
    return errors


def prune_images(s3, build, images_to_keep, dry_run, bucket, prefix):
    images_from_meta_json = build.meta_json.get("images", [])
    # Get the image names and paths currently in meta.json
    current_images_data = [(name, data.get("path")) for name, data in images_from_meta_json.items()]
    errors = []

    for name, path in current_images_data:
        if name not in images_to_keep:
            image_prefix = os.path.join(prefix, f"{build.id}/{build.arch}/{path}")
            if dry_run:
                print(f"Would prune {bucket}/{image_prefix}")
            else:
                try:
                    s3.delete_object(Bucket=bucket, Key=image_prefix)
                    print(f"Pruned {name} image for {build.id} for {build.arch}")
                except botocore.exceptions.ClientError as e:
                    if e.response['Error']['Code'] == 'NoSuchKey':
                        print(f"{bucket}/{image_prefix} already pruned.")
                    else:
                        errors.append(e)
    if errors:
        print(f"Found errors when pruning images for {build.id}:")
        for e in errors:
            print(e)
        raise Exception("Some errors were encountered")


def prune_build(s3_client, bucket, prefix, build_id, dry_run):
    build_prefix = os.path.join(prefix, f"{build_id}/")
    if dry_run:
        print(f"Would delete all resources in {bucket}/{build_prefix}.")
    else:
        try:
            # List all objects under the specified prefix
            objects_to_delete = s3_client.list_objects_v2(Bucket=bucket, Prefix=build_prefix)
            if 'Contents' in objects_to_delete:
                # Extract the object keys and format them for deletion
                delete_keys = [{'Key': obj['Key']} for obj in objects_to_delete['Contents']]
                # Delete objects
                s3_client.delete_objects(Bucket=bucket, Delete={'Objects': delete_keys})
                print(f"Pruned {build_id} completely from {bucket}/{build_prefix}.")
            else:
                print(f"No objects found to delete in {bucket}/{build_prefix}.")
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                print(f"{bucket}/{build_prefix} already pruned or doesn't exist.")
            else:
                raise Exception(f"Error pruning {build_id}: {e.response['Error']['Message']}")


if __name__ == "__main__":
    main()
